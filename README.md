# ğŸš€ **RAG ChatBot for Research Papers**

*A Retrieval-Augmented Generation (RAG) system for querying academic PDFs using LLMs + Vector Databases*

---

## ğŸ“š **Overview**

This project is an end-to-end **RAG (Retrieval-Augmented Generation) ChatBot** built to answer questions **based on your own research papers and academic PDFs**.

Unlike ChatGPTâ€”which does *not* know your private documentsâ€”this chatbot:

* Loads your PDFs ğŸ“„
* Splits them into meaningful chunks âœ‚ï¸
* Embeds them using vector embeddings ğŸ”¢
* Stores them in a vector database (ChromaDB) ğŸ—„ï¸
* Retrieves the most relevant chunks for any user query ğŸ”
* Feeds those chunks into an LLM (OpenAI GPT-4o) ğŸ¤–
* Generates accurate, context-grounded responses ğŸ§ 

This makes it ideal for **students, researchers, and professionals** who need to extract insights from large collections of academic papers.

---

## âœ¨ **Features**

### ğŸ” **Accurate Retrieval**

The system searches your papers using semantic similarity â€” not keyword search.

### ğŸ“˜ **Research-aware Chatbot**

It answers only using **information found inside your documents**, preventing hallucinations.

### ğŸ¯ **Clean & Human-Friendly Responses**

A custom conversational prompt makes the assistant sound natural, clear, and concise.

### ğŸ§© **Metadata-Rich Sources**

Each answer displays **which paper** and **which page** the information was taken from.

### ğŸ–¥ï¸ **Streamlit Frontend UI**

A simple but effective UI lets users:

* Enter questions
* View answers
* See source citations

### ğŸ—‚ï¸ **Modular Code Structure**

Includes:

* `app/rag.py` â†’ RAG logic
* `app/vectorstore.py` â†’ Ingestion + embeddings
* `frontend/interface.py` â†’ Streamlit UI
* `app/config.py` â†’ Environment + paths

### ğŸ” **Environment-Safe Configuration**

Uses a `.env` file to securely load API keys using `python-dotenv`.

---

## ğŸ§  **Architecture Diagram**

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    PDFs       â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      Load & Split into Chunks
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Embeddings    â”‚  â† OpenAI Embeddings (text-embedding-3-small)
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      Store in Vector DB
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚  ChromaDB    â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚
     Similarity Search (top_k)
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Retrieved Docs â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
     Feed Into LLM w/ Custom Prompt
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   GPT-4o       â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
          Final Answer
```

---

## ğŸ“¦ **Tech Stack**

### **Backend**

* Python 3.12
* LangChain ğŸ¦œ
* ChromaDB
* OpenAI GPT-4o
* python-dotenv

### **Frontend**

* Streamlit

### **Utilities**

* PyPDFLoader (LangChain)
* RecursiveCharacterTextSplitter

---

## ğŸ› ï¸ **How It Works (Step-by-Step)**

### **1ï¸âƒ£ PDF Loading**

The system loads all PDF files from a configured directory:

```python
pages = PyPDFLoader(path).load()
```

### **2ï¸âƒ£ Chunking**

Long text is broken into overlapping chunks for better retrieval:

```python
RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
```

### **3ï¸âƒ£ Embedding**

Each chunk is converted into a vector using OpenAI Embeddings.

### **4ï¸âƒ£ Vector Storage**

Embeddings + metadata are saved inside **ChromaDB** for persistent storage.

### **5ï¸âƒ£ Retrieval**

When the user asks a question:

* The question is embedded
* Similarity search retrieves the most relevant chunks

### **6ï¸âƒ£ RAG Prompting**

Chunks are inserted into a custom prompt that ensures the AI:

* Uses only the provided context
* Speaks conversationally
* Does not hallucinate

### **7ï¸âƒ£ Answer Generation**

The LLM (GPT-4o) produces a natural, reader-friendly reply.

---

## ğŸ¨ **Frontend Preview (Streamlit)**

The minimal UI provides:

* ğŸ” A question input box
* âœï¸ A formatted answer
* ğŸ“„ Source citations (paper titles + pages)

Run it with:

```bash
streamlit run frontend/interface.py
```

---

## ğŸ§ª **Running the Project Locally**

### **1. Clone the Repo**

```bash
git clone https://github.com/<your-username>/RAG-Chat-Bot.git
cd RAG-Chat-Bot
```

### **2. Create a Virtual Environment**

```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
```

### **3. Install Dependencies**

```bash
pip install -r requirements.txt
```

### **4. Create a `.env` File**

```
OPENAI_API_KEY=your_key_here
```

### **5. Run Ingestion (Build Vector DB)**

```bash
python -m app.rag
```

### **6. Launch UI**

```bash
streamlit run frontend/interface.py
```

---

## ğŸ“Œ **Project Structure**

```
RAG-Chat-Bot/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py          # Environment variables + paths
â”‚   â”œâ”€â”€ rag.py             # RAG chain + LLM logic
â”‚   â”œâ”€â”€ vectorstore.py     # PDF loading, chunking, embeddings
â”‚   â””â”€â”€ cli.py             # Terminal interface
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ interface.py       # Streamlit chatbot UI
â”‚
â”œâ”€â”€ vector_database/       # Persistent Chroma store
â”œâ”€â”€ .env                   # API key
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ¯ **Why This Project Is Useful**

This RAG Chatbot can be used for:

* ğŸ“˜ Summarizing research papers
* ğŸ“ Extracting methodologies, results, and insights
* ğŸ“ Study assistance
* ğŸ”¬ Literature review automation
* ğŸ§ª Academic research exploration

And can be extended to:

* Corporate document search
* Policy Q&A systems
* Legal document assistants
* Medical document explorers

